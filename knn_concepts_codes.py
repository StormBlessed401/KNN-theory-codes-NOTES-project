# -*- coding: utf-8 -*-
"""KNN_Concepts_Codes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A3PWh0V555U0E7mjvuF0aMcsrMreDat-
"""

'''StandardScaler:
-->StandardScaler is applied to make all features have the same scale, so that ML models like KNN or SVM don't get biased by larger numbers.* -->StandardScaler rescales your features so they have zero mean and unit variance, helping models like KNN, SVM, and Logistic Regression perform better. i.e., it transforms your data so that each feature has mean = 0 and standard deviation = 1. QQ.Why do we use it? -->Because many machine learning algorithms: -->Work better when features are on the same scale -->Are sensitive to the magnitude of input features (like KNN, SVM, Logistic Regression, Gradient Descent)

Given a feature column, it transforms each value like this:
z=(x-μ)/σ Where: x = original value μ = mean of the column σ = standard deviation

After scaling: The column will have mean ≈ 0 Standard deviation ≈ 1

EXAMPLE:
Original Value--> Height (cm): [150, 160, 170, 180, 190] After std scaler--> [-1.41, -0.71, 0.0, 0.71, 1.41]

Normalization and Standardization
-->Normalization: compresses all values between 0 and 1 → useful when features have different units or scales -->Standardization: centers and spreads data → useful when data needs normal distribution behavior Both make features comparable in magnitude, but use different techniques. Choice depends on the algorithm you're using.

Normalization Formula (Min-Max Scaling):
X(normalized) = X-X(min)/X(max)-X(min)

Example:
If a feature has values: [10, 20, 30]; X(min)=10, X(max)=30 So FOR X=20: X(normalized)= 20-30/30-10 = 10/20 = 0.5'''

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

'''Purpose of splitting data into train and test:
To teach the model using the training data, and then check how well it performs on new, unseen data (test set). This helps us measure how accurately the model can make real-world predictions.

Underfitting & Overfitting (Simple Explanation + KNN Relation)
-->Underfitting When a model is too simple to learn the data properly. -->It performs poorly on both training and test data. Example: Using very few neighbors in KNN (like k = 1) might not capture the broader patterns in data.

-->Overfitting When a model is too complex and learns even the noise in the training data. -->It performs very well on training data but poorly on test data. Example: Using a very large number of neighbors in KNN (like k = total_samples) may cause the model to generalize too much and miss fine details.

-->Relation to KNN Low value of k → Risk of overfitting (model too sensitive).

-->High value of k → Risk of underfitting (model too generalized).
-->Choosing the right k is key to balancing bias and variance.

Limitations of KNN:
-->Slow for large datasets (because it computes distance from every point in the dataset).
-->Sensitive to irrelevant features (because all features contribute equally unless scale).
-->Affected by feature scaling (distance-based algorithm, so features must be normalized or standardized).
-->Struggles with high dimensional data (curse of dimensionality weakens distance metrics).
-->No model training (lazy learning) (leads to high prediction time and memory usage).
 -->Sensitive to outliers (outliers can heavily influence nearest neighbors).'''

import numpy as np
import pandas as pd

import os
import kagglehub

# Download latest version
path = kagglehub.dataset_download("uciml/breast-cancer-wisconsin-data")

print("Path to dataset files:", path)
df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')
df.head()

df.drop(columns=['Unnamed: 32', 'id'], inplace=True)
df.head()

df.shape

X=df.drop(['diagnosis'], axis=1)
y=df['diagnosis']
from  sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)

knn.fit(X_train,y_train)

from sklearn.metrics import accuracy_score
y_pred = knn.predict(X_test)
accuracy_score(y_test, y_pred)

scores = []
for i in range(1,16):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    y_pred = knn.predict(X_test)
    scores.append(accuracy_score(y_test, y_pred))

import matplotlib.pyplot as plt
plt.plot(range(1,16),scores)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
from sklearn.preprocessing import StandardScaler
from ipywidgets import interact, fixed

def load_data():
    cancer = datasets.load_breast_cancer()
    return cancer

def plot_decision_boundaries(n_neighbors, data, labels):
    h = .02
    cmap_light = ListedColormap(['orange', 'blue'])
    cmap_bold = ListedColormap(['darkorange', 'darkblue'])

    clf = neighbors.KNeighborsClassifier(n_neighbors)
    clf.fit(data, labels)

    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1
    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1

    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(8, 6))
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=cmap_bold, edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(f'2-Class classification (k = {n_neighbors})')
    plt.show()

cancer = load_data()

# Use only the first two features and standardize them.
X = StandardScaler().fit_transform(cancer.data[:, :2])
y = cancer.target

# Interactive widget
interact(plot_decision_boundaries, n_neighbors=(1, 20), data=fixed(X), labels=fixed(y));